{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KixHYy9Y5Tu"
      },
      "source": [
        "# Lab 3: Pytorch Operators and Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaRkxg62ZH3e"
      },
      "source": [
        "## Assignment: Fashion MNIST Classification\n",
        "\n",
        "In Lab3, we will use the Fashion MNIST dataset (see video recording for explanation of Fashion MNIST dataset). Similar to previous Lab2, you should design a fully-connected network as well as the pytorch based training/validation/testing process on the classification task. But differently, we will dive into the optimizers, operations or modules that we have learned in the lectures, and to see how they can benefit the capacity of your model.\n",
        "\n",
        "\n",
        "\n",
        "1. Try different optimizers including RMSProp, Adam and SGD(You can find the corresponding functions in the torch.optim library). Log your training loss and test accuracy. Comparing these optimizers, how do they work? Which one is the best for this task and try to explain it.\n",
        "\n",
        "2. Analyze the overfitting/underfitting situation of your model. Try to use regularization like L2/L1, Dropout layers, etc. to improve your performance. How does your regularization work? And how do you balance your regularization and the loss optimization?\n",
        "\n",
        "3. Try different initialization ways like random normal, Xavier and He (Kaiming) etc. How can these initialization affect your training process and performance? \n",
        "\n",
        "4. Try normalization like batch normalization or layer normalization, whether these normalization methods can help your training or improve your performance?\n",
        "\n",
        "5. To further improve the performance, you can also choose your own hyperparameters, including:\n",
        "- Number of layers\n",
        "- Number of neurons in each layer\n",
        "- Learning rate\n",
        "- Number of training epochs\n",
        "\n",
        "For the comparing of different optimizers, regularization, initialization, normalization and hyper-parameters, you are recommended to form a table of the comparison results.  Report your loss as ”loss curve” and accuracy for different settings and draw conclusions. **You can split a small validation set from the training set to help you analyze the effectiveness of different modules better.**\n",
        "\n",
        "For the convenient implementation of different optimizers, regularization, initialization and normalization, you can always check the tutorial of pytorch library (https://pytorch.org/tutorials/) for some useful funuctions and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LHdZ46lmW8ZH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "train_batch_size =  # Define train batch size\n",
        "test_batch_size  =  # Define test batch size (can be larger than train batch size)\n",
        "\n",
        "\n",
        "# Use the following code to load and normalize the dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.FashionMNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=test_batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt6scrFsYNse"
      },
      "outputs": [],
      "source": [
        "#Define your network:\n",
        "class Network(nn.Module):\n",
        "  def __init__(self): #Can provide additional inputs for initialization\n",
        "    #Define the network layer(s) and activation function(s)\n",
        "\n",
        "  def forward(self, input):\n",
        "    #How does your model process the input?\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQwTLOTiXnso"
      },
      "outputs": [],
      "source": [
        "# Define your optimizer\n",
        "model = Network()\n",
        "optimizer =\n",
        "epochs =\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for train_sample in train_loader:\n",
        "    #Calculate training loss on model\n",
        "\n",
        "# Calculate accuracy on test set"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab 3 Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
